---
title: "Importing Text From Documents"
author: "Jason Freels"
date: "10/9/2018"
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: 'css/logm655.css'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

## Overview

This document walks through the process of extracting text data from documents.  It is assumed that the documents from which the data will be extracted are saved using one of the following formats:

1. Plain text - i.e. notepad `.txt` files or a similar format
2. Word Document
3. PDF document - saved either as a native PDF file OR as a scanned image

There are a number of R packages and/or 3^rd^ party tools that can be used for this purpose.  I'll demo these tools by extracting the text from the `supreme_court_opinions_2017_sample` folder, note that all of these files are saved using the PDF format.

## Extracting textual data from PDF documents

The documents from which the text will be extracted are the United States Supreme Court Opinions from 2017.  This documents can be accessed from [**here**](https://www.supremecourt.gov/opinions/slipopinion/17).  There are a total of 85 documents in this corpus.  Rather than download each of these documents separately, I prefer to let R do this for me, you can too by using the code shown in the next section.  First, install and load the necessary packages.  The `pacman` package is a helpful package for simplifying the process of getting packages, let's install it first.

```{r, eval=FALSE}
install.packages('pacman')
```

With the `pacman` package installed, we can easily install and load other packages we'll need to download the files from the web.

```{r, eval=FALSE}
pacman::p_load(XML,
               rvest,
               RCurl,
               rprojroot,
               tesseract)
```

### Saving local copies of the documents 

With these packages installed, we can now download the documents from the url using the code shown below.  <u>**However, do not run this code as it is written in the LOGM655 project - it will overwrite the existing files.  I'll show you what to change**</u>

```{r, eval=FALSE}
# Assign the URL to a text string
url_root  <- 'https://www.supremecourt.gov'
     url  <- 'https://www.supremecourt.gov/opinions/slipopinion/17'
     
# Assign the root of the project this
# helps locate where to save the files
# Before going forward you should change 
# these values to a location on your machine
proj_root   <- find_root(is_rstudio_project)
save_folder <- file.path(proj_root,'raw_data_files','supreme_court_opinions_2017')

# Extract html/xml content from URL
rcurl.doc <- RCurl::getURL(url, 
                           .opts = RCurl::curlOptions(followlocation = TRUE))

# Parse html content 
url_parsed <- XML::htmlParse(rcurl.doc, asText = TRUE)

# We need to get the href attributes from 
# the anchor tags <a> stored in the table 
# as table data tags <td>
# First, let's get all of the attributes
attrs <- XML::xpathApply(url_parsed, "//td//a", XML::xmlAttrs)

# Next, we'll split out the hrefs 
# from the other attributes
hrefs <- sapply(seq_along(attrs), FUN = function(x) attrs[[x]][['href']])

# Then, we only want the hrefs for the files 
# that have a .pdf file extension
pdfs  <- hrefs[tools::file_ext(hrefs) == 'pdf']

# Construct a list of URL's for each file
# by pasting two character strings together  
files <- paste0(url_root,pdfs)

# loop through each element in the files
# vector and download the file to destfile
for(i in files) {
  
  filename <- basename(i)
  download.file(i, 
                destfile = file.path(save_folder,filename),
                method = 'curl')
  
}
```

Now that we have these document downloaded locally, I'm going to use a small set of these documents `supreme_court_opinions_2017_sample` folder. Before we get too far, let's install some helpful packages.

```{r}
pacman::p_load(qdapTools,
               pdftools,
               antiword,
               glue)
```

### Using the XpdfReader

First, let's use XpdfReader by letting R talk to our system (i.e. command prompt or terminal).  Xpdf is an open source project developed by [**Glyph & Cog**](http://www.glyphandcog.com/) for viewing Portable Document Format (PDF)
files.  The Xpdf project also includes **xpdf tools** which contain the following utilities that are useful for extracting data from pdf files:

- pdftotext - Convert a PDF file to text
- pdftohtml - Convert a PDF file to HTML
- pdfinfo - Dump a PDF file's Info dictionary (plus some other useful information)
- pdffonts - List the fonts used in a PDF file and various information for each font
- pdfdetach - List or extract embedded files (attachments) from a PDF file
- pdftoppm - Convert a PDF file to a series of PPM/PGM/PBM-format bitmaps
- pdftopng - Convert a PDF file to a series of PNG image files
- pdfimages - Extract the images from a PDF file

To use these utilities we must first download xpdf and xpdf tools from [**this site**](http://www.xpdfreader.com/).  After downloading and unzipping xpdf tools, make sure to note the file location where it was saved.  On my machine, the main xpdf folder is located at

```{r}
xpdf_tools <- 'C:/Program Files/xpdf-tools-win-4.00/bin64'
```

Next we'll create a character vector containing the names of the files in the `supreme_court_opinions_2017_sample` folder.

```{r}
library(rprojroot)
root <- find_root(is_rstudio_project)

dest <- file.path(root, 'raw_data_files', 'supreme_court_opinions_2017_sample')

# make a vector of PDF file names
my_files <- list.files(path = dest, 
                       pattern = "pdf",  
                       full.names = TRUE)

my_files
```

Now, let's use the `pdftotext` utility to extract the text from the first document in `my_files`.  The `pdftotext` utility converts each PDF file in into a text file.  By default, the text file is created in the same directory as the PDF file.  Since this is a command-line utility we need to construct the commands to send, normally this would look like the code below.

```{r}
if(nchar(Sys.which('pdftotext') > 0)) {
  
   system('pdftotext')

}
```

To have R call pdftotext, we need to paste (or glue) these four separate charater strings together into a single command.  This can be done as shown below. 

```{r}
cmd1 <- 'pdftotext' # which utility are we calling?
cmd2 <- ''          # which options? - here we use none
cmd3 <- my_files[1] # which file to convert
cmd4 <- ''          # which file to write to

# Two options to connect the strings
CMD1 <- glue::glue("{cmd1} {cmd2} {cmd3} {cmd4}")
CMD2 <- paste(cmd1, cmd2, cmd3, cmd4, sep = ' ')
```

Now, we send this command to either the command prompt or terminal, depending on the type of OS being used.  

```{r, eval=FALSE}
system(CMD1)
```

To have `pdftotext` do this action recursively for each of the files we can run the above command in a loop or use one of the apply functions, in this case `lapply()` 

```{r}
lapply(my_files, 
       FUN = function(x) system(glue::glue("pdftotext {x}"), wait = FALSE))
```

Now that the data has been extracted into a text file it can then be read into R as a character vector using the `readLines()` function. Note that each element in character vector is a line of text.  A line of text in defined by a certain number of characters, this number of characters may not coincide with the number of characters on the original pdf document.  If we desire to maintain the exact layout of the document we can specify the option `-layout` to maintain the original layout of the text.  

```{r}
text_files <- list.files(path = dest, 
                         pattern = "txt",  
                         full.names = TRUE)

text1 <- readLines(con = text_files[1])
text1[1:50]
```

The remaining utilities in xpdf tools can be used in a similar manner as to what what shown here for `pdftotext`.

### Using the `pdftools` package

The pdftools package is an R interface to the Poppler C++ API. The package makes it easy to use several utilities based on 'libpoppler' for extracting text, fonts, attachments and metadata from a PDF file. The package also supports high quality rendering of PDF documents info PNG, JPEG, TIFF format, or into raw bitmap vectors for further processing in R. The `pdftools` contains the following user-level functions 

```{r}
data.frame(function_names = getNamespaceExports('pdftools'))
```

that can be used to retreive information in a similar fashion as to what was shown for the xpdf tools.  As example using the `pdf_text()` function extracts the text from a PDF file as a character vector. Note that in this case each character vector is an entire page. 

```{r}
text2 <- pdftools::pdf_text(pdf = my_files[1])
text2[1]
```

The `pdftools` package does not contain a function to save the text directly to a text file, however we can write to a text file using the `writeLines()` function. A downside to this is that we must keep track of the name of the original document when specifying a name to the new `.txt` file. This could lead to errors in assigning the wrong name to a file. Also, note that the output file contains a blank line between each line of text.  This may not present a problem depending on the desired output, however it presents an extra step in our data preparation process.

```{r eval=FALSE}
writeLines(text = text2,
           con = gsub('pdf','txt',file.path(dest,basename(my_files[1]))))
```

## Extracting textual data from MS^&#174;^ Word documents

Once again, we need to download the Word files from the web, which can be done using the code below. <u>**Again, do not run this code as it is written in the LOGM655 project - it will overwrite the existing files.  I'll show you what to change**</u>

```{r, eval=FALSE}
# Assign the URL to a text string
url_root  <- 'http://hhoppe.com/'
     url  <- 'http://hhoppe.com/microsoft_word_examples.html'
     
# Assign the root of the project this
# helps locate where to save the files
# Before going forward you should change 
# these values to a location on your machine
proj_root   <- find_root(is_rstudio_project)
save_folder <- file.path(proj_root,'raw_data_files','msword_document_examples')

# Extract html/xml content from URL
rcurl.doc <- RCurl::getURL(url, 
                           .opts = RCurl::curlOptions(followlocation = TRUE))

# Parse html content 
url_parsed <- XML::htmlParse(rcurl.doc, asText = TRUE)

# We need to get the href attributes from 
# the anchor tags <a> stored on the page 
attrs <- XML::xpathApply(url_parsed, "//a", XML::xmlAttrs)

# Next, we'll split out the hrefs 
# from the other attributes
hrefs <- sapply(seq_along(attrs), FUN = function(x) attrs[[x]][['href']])

# Then, we only want the hrefs for the files 
# that have a .docx file extension
docx  <- hrefs[tools::file_ext(hrefs) == 'docx']

# Construct a list of URL's for each file
# by pasting two character strings together  
files <- paste0(url_root, docx)

# loop through each element in the files
# vector and download the file to destfile
for(i in files) {
  
  filename <- basename(i)
  download.file(i, 
                destfile = file.path(save_folder,filename),
                method = 'curl')
  
}
```

### Using the `qdapTools` package

The `qdapTools` package is a collection of tools associated with the `qdap` package that may be useful outside of the context of text analysis. In the R language there are 4 packages in the `qdap` family used for qualitative data analysis

- `qdap` - Bridging the Gap Between Qualitative Data and Quantitative Analysis
- `qdapTools` - Tools for the 'qdap' Package
- `qdapRegex` - Regular Expression Removal, Extraction, and Replacement Tools
- `qdapDictionaries` - Dictionaries and Word Lists for the 'qdap' Package

Within the `qdapTools` package is the `read_docx()` function that is used for (you'll never guess) reading `.docx` files.  First, let's list the files like we did for the PDF files before.

```{r}
library(rprojroot)
root <- find_root(is_rstudio_project)

dest <- file.path(root, 'raw_data_files', 'msword_document_examples')

# make a vector of PDF file names
ms_files <- list.files(path = dest, 
                       pattern = "docx",  
                       full.names = TRUE)

ms_files
```

Now, let's read in the content of the first file in the `ms_files` vector which is <u>`r ms_files[1]`</u>.

```{r}
docx1 <- qdapTools::read_docx(file = ms_files[1])
docx1[90]
```

Here we see that this function extracts the text as paragraphs, rather than as lines or pages.


## OCR

```{r}
 pngfile <- pdftools::pdf_convert(fle, dpi = 600)
```

```{asis}
Converting page 1 to Space Launch_94-05_R_1.png... done!
Converting page 2 to Space Launch_94-05_R_2.png... done!
Converting page 3 to Space Launch_94-05_R_3.png... done!
Converting page 4 to Space Launch_94-05_R_4.png... done!
Converting page 5 to Space Launch_94-05_R_5.png... done!
Converting page 6 to Space Launch_94-05_R_6.png... done!
Converting page 7 to Space Launch_94-05_R_7.png... done!
Converting page 8 to Space Launch_94-05_R_8.png... done!
Converting page 9 to Space Launch_94-05_R_9.png... done!
Converting page 10 to Space Launch_94-05_R_10.png... done!
Converting page 11 to Space Launch_94-05_R_11.png... done!
Converting page 12 to Space Launch_94-05_R_12.png... done!
Converting page 13 to Space Launch_94-05_R_13.png... done!
Converting page 14 to Space Launch_94-05_R_14.png... done!
Converting page 15 to Space Launch_94-05_R_15.png... done!
Converting page 16 to Space Launch_94-05_R_16.png... done!
Converting page 17 to Space Launch_94-05_R_17.png... done!
Converting page 18 to Space Launch_94-05_R_18.png... done!
Converting page 19 to Space Launch_94-05_R_19.png... done!
Converting page 20 to Space Launch_94-05_R_20.png... done!
Converting page 21 to Space Launch_94-05_R_21.png... done!
Converting page 22 to Space Launch_94-05_R_22.png... done!
Converting page 23 to Space Launch_94-05_R_23.png... done!
Converting page 24 to Space Launch_94-05_R_24.png... done!
Converting page 25 to Space Launch_94-05_R_25.png... done!
Converting page 26 to Space Launch_94-05_R_26.png... done!
Converting page 27 to Space Launch_94-05_R_27.png... done!
Converting page 28 to Space Launch_94-05_R_28.png... done!
Converting page 29 to Space Launch_94-05_R_29.png... done!
Converting page 30 to Space Launch_94-05_R_30.png... done!
Converting page 31 to Space Launch_94-05_R_31.png... done!
Converting page 32 to Space Launch_94-05_R_32.png... done!
Converting page 33 to Space Launch_94-05_R_33.png... done!
Converting page 34 to Space Launch_94-05_R_34.png... done!
Converting page 35 to Space Launch_94-05_R_35.png... done!
Converting page 36 to Space Launch_94-05_R_36.png... done!
Converting page 37 to Space Launch_94-05_R_37.png... done!
Converting page 38 to Space Launch_94-05_R_38.png... done!
Converting page 39 to Space Launch_94-05_R_39.png... done!
Converting page 40 to Space Launch_94-05_R_40.png... done!
Converting page 41 to Space Launch_94-05_R_41.png... done!
Converting page 42 to Space Launch_94-05_R_42.png... done!
Converting page 43 to Space Launch_94-05_R_43.png... done!
Converting page 44 to Space Launch_94-05_R_44.png... done!
Converting page 45 to Space Launch_94-05_R_45.png... done!
Converting page 46 to Space Launch_94-05_R_46.png... done!
Converting page 47 to Space Launch_94-05_R_47.png... done!
Converting page 48 to Space Launch_94-05_R_48.png... done!
Converting page 49 to Space Launch_94-05_R_49.png... done!
Converting page 50 to Space Launch_94-05_R_50.png... done!
Converting page 51 to Space Launch_94-05_R_51.png... done!
Converting page 52 to Space Launch_94-05_R_52.png... done!
Converting page 53 to Space Launch_94-05_R_53.png... done!
Converting page 54 to Space Launch_94-05_R_54.png... done!
Converting page 55 to Space Launch_94-05_R_55.png... done!
Converting page 56 to Space Launch_94-05_R_56.png... done!
Converting page 57 to Space Launch_94-05_R_57.png... done!
Converting page 58 to Space Launch_94-05_R_58.png... done!
Converting page 59 to Space Launch_94-05_R_59.png... done!
Converting page 60 to Space Launch_94-05_R_60.png... done!
Converting page 61 to Space Launch_94-05_R_61.png... done!
Converting page 62 to Space Launch_94-05_R_62.png... done!
Converting page 63 to Space Launch_94-05_R_63.png... done!
Converting page 64 to Space Launch_94-05_R_64.png... done!
Converting page 65 to Space Launch_94-05_R_65.png... done!
Converting page 66 to Space Launch_94-05_R_66.png... done!
Converting page 67 to Space Launch_94-05_R_67.png... done!
Converting page 68 to Space Launch_94-05_R_68.png... done!
Converting page 69 to Space Launch_94-05_R_69.png... done!
Converting page 70 to Space Launch_94-05_R_70.png... done!
Converting page 71 to Space Launch_94-05_R_71.png... done!
Converting page 72 to Space Launch_94-05_R_72.png... done!
Converting page 73 to Space Launch_94-05_R_73.png... done!
Converting page 74 to Space Launch_94-05_R_74.png... done!
Converting page 75 to Space Launch_94-05_R_75.png... done!
Converting page 76 to Space Launch_94-05_R_76.png... done!
Converting page 77 to Space Launch_94-05_R_77.png... done!
Converting page 78 to Space Launch_94-05_R_78.png... done!
Converting page 79 to Space Launch_94-05_R_79.png... done!
Converting page 80 to Space Launch_94-05_R_80.png... done!
Converting page 81 to Space Launch_94-05_R_81.png... done!
Converting page 82 to Space Launch_94-05_R_82.png... done!
Converting page 83 to Space Launch_94-05_R_83.png... done!
Converting page 84 to Space Launch_94-05_R_84.png... done!
Converting page 85 to Space Launch_94-05_R_85.png... done!
Converting page 86 to Space Launch_94-05_R_86.png... done!
Converting page 87 to Space Launch_94-05_R_87.png... done!
Converting page 88 to Space Launch_94-05_R_88.png... done!
Converting page 89 to Space Launch_94-05_R_89.png... done!
Converting page 90 to Space Launch_94-05_R_90.png... done!
Converting page 91 to Space Launch_94-05_R_91.png... done!
Converting page 92 to Space Launch_94-05_R_92.png... done!
Converting page 93 to Space Launch_94-05_R_93.png... done!
Converting page 94 to Space Launch_94-05_R_94.png... done!
Converting page 95 to Space Launch_94-05_R_95.png... done!
Converting page 96 to Space Launch_94-05_R_96.png... done!
Converting page 97 to Space Launch_94-05_R_97.png... done!
Converting page 98 to Space Launch_94-05_R_98.png... done!
Converting page 99 to Space Launch_94-05_R_99.png... done!
Converting page 100 to Space Launch_94-05_R_100.png... done!
Converting page 101 to Space Launch_94-05_R_101.png... done!
Converting page 102 to Space Launch_94-05_R_102.png... done!
Converting page 103 to Space Launch_94-05_R_103.png... done!
Converting page 104 to Space Launch_94-05_R_104.png... done!
Converting page 105 to Space Launch_94-05_R_105.png... done!
Converting page 106 to Space Launch_94-05_R_106.png... done!
Converting page 107 to Space Launch_94-05_R_107.png... done!
Converting page 108 to Space Launch_94-05_R_108.png... done!
Converting page 109 to Space Launch_94-05_R_109.png... done!
Converting page 110 to Space Launch_94-05_R_110.png... done!
Converting page 111 to Space Launch_94-05_R_111.png... done!
Converting page 112 to Space Launch_94-05_R_112.png... done!
Converting page 113 to Space Launch_94-05_R_113.png... done!
Converting page 114 to Space Launch_94-05_R_114.png... done!
Converting page 115 to Space Launch_94-05_R_115.png... done!
Converting page 116 to Space Launch_94-05_R_116.png... done!
Converting page 117 to Space Launch_94-05_R_117.png... done!
Converting page 118 to Space Launch_94-05_R_118.png... done!
Converting page 119 to Space Launch_94-05_R_119.png... done!
Converting page 120 to Space Launch_94-05_R_120.png... done!
Converting page 121 to Space Launch_94-05_R_121.png... done!
Converting page 122 to Space Launch_94-05_R_122.png... done!
```

```{r}
text <- tesseract::ocr(pngfile)
```

```{asis}
First use of Tesseract: copying language data...
```


## Other Datasets

There are many other sources of publicly available text data for use in training NLP models or to test out some of the basic NLP tasks.  A few sources of such data are listed below.  

1. [**kaggle**](https://www.kaggle.com/datasets?sortBy=relevance&group=all&search=text)
2. [**UCSD**](https://ucsd.libguides.com/data-statistics/textmining)
3. [**QUANDL**](https://www.researchgate.net/deref/https%3A%2F%2Fwww.quandl.com%2F) 
4. [**kdnuggets**](https://www.kdnuggets.com/datasets/index.html) 
5. [**Amazon reviews**](https://snap.stanford.edu/data/web-Amazon.html) 
6. [**ENRON emails**](https://www.cs.cmu.edu/~./enron/) 
7. [**Hillary Clinton's declassified emails**](http://www.readhillarysemail.com)
8. [**R package: Harry Potter**](https://github.com/bradleyboehmke/harrypotter)