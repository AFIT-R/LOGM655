---
title: "Final Project LOGM655"
author: "2nd Lt. Dom Speranza"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
abstract: 'The orignial triliogy of Star Wars (Episodes IV, V, VI) are some of the most iconic movies of science fiction.  Back in the late 1970s, the special effects and story line used would set the stage for hundreds of science fiction movies.  For this final prject, we will be examining the written script of the Star Wars movies. Through these written scripts, we are able to gain basic analysis of movie based on more frequently used words, a brief summary from important parts in each of the movies, and identify the major charaters and entities in the movie'

---

#Project Overview
   In this Project we will be analysing the movie scripts for StarWars the original trilogy (episodes IV, V, VI). First off, we will look at basic word analysis for each of the scripts. This will include data scrubbing, word frequency, and charts that will illustrate common words used throughout the script. Next, we will be using the package Lexrankr to try and develop a summary of each of the movie scripts and see how the software does in summarizing the movie scripts. This technique is extractive with limitations that will be addressed.  Finally, we will use packages NLP and openNLP in R to try and extract all of the charaters and entities from the star wars scripts. A combination of these three techniques will provide an indepth analysis of the original trilogy of Star Wars.  


## Get Nessasary Packages for Analysis
   Before we get into the data cleaning, we are going to load in any appliciable packages needed for reading in the document along with any packages that might be used later on. This way we will not need to constantly be loading and installing packages and we can focus strictly on the analysis. There might be some packages that are not used.   

```{r,include = TRUE}
#read in the movie scripts'
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(tm, 
               pdftools, 
               here,
               tau,
               tidyverse,
               stringr,
               tidytext, 
               RColorBrewer,
               qdap,
               qdapRegex,
               qdapDictionaries,
               qdapTools,
               data.table,
               coreNLP,
               scales,
               text2vec,
               SnowballC,
               DT,
               quanteda,
               RWeka,
               broom,
               tokenizers,
               grid,
               knitr,
               widyr,
               ggplot2,
               readr,
               lexRankr,
               textrank,
               openNLP,
               rJava,
               magrittr,
               readr,
               openNLPmodels.en,
               NLP)

```

#Methodology

 This section will go into a description of the data, the cleaning of the data for the various packages, and the analysis done on the data.  

##Where to get the Data

 This dataset was taken from Kaggle where several various datasets can be taken from.  The data included 3 text documents. Each document is the script for a star wars movie (episodes 4,5 and 6).  The original format of these text files begin each line of script with a number (indicated which line number that was), followed by the charater who is saying the line, and finally the sentence for what is actually said.  There is no filler inbetween the lines said. No setting or description of what is going on aside from what is being said can be gathered from the data. This will prove some portions of analysis difficult to accomplish. 
 
 Because each package seems to need the data in a slightly different format, we will do data cleaning and formatting befoe each of the sections.  

#Analysis
##Initial Look at the Scripts
###Pulling in the Data and Cleaning

   Now we will read in the movies scripts in and merge them into one continuous string with identifiers to determine which movie they came from. This will be helpful in identifying the word frequencies for each of the individual movie scrips. This will also be helpful in the text summarization portion.  

```{r}
#pull in movies with tiddyverse This will create a tibble. 
### Note ###
# You will need to set your own working directory in order for this to fun properly 
files<-list.files("Files", pattern = '.txt',full.names = T)
sw_tidy<-tibble::tibble()

#This will read in each of the movie scripts and get rid of the \\ in front of every line in the text
Episode4<-read_file(files[1])
Episode4<-gsub('\\\"'," ",Episode4)
Episode5<-read_file(files[2])
Episode5<-gsub('\\\"'," ",Episode5)
Episode6<-read_file(files[3])
Episode6<-gsub('\\\"'," ",Episode6)

#makes titles as markers for later use
titles <- c("Episode 4", 
            "Episode 5", 
            "Episode 6")

#creates a single list of the movies
movies<-list(Episode4,
             Episode5,
             Episode6)

#runs through each of the movies and seperates it out by each word and which movie that word came from. All of the words are in order.
for(i in seq_along(titles)) {
  
  clean <- tibble::tibble(text = movies[[i]]) %>%
    tidytext::unnest_tokens(word, text) %>%
    dplyr::mutate(movie = titles[i]) %>%
    dplyr::select(movie, dplyr::everything())
  sw_tidy <- base::rbind(sw_tidy, clean)
} 

```

   Now that all the scripts are read into a format we can work with, we are able to use the dplyr package to both remove stop words and see which words are used the most in the script. Because each line in the text begins with the charaters name who said the line, it is likely that names of charaters will pop up in the most frequently used words in the script itself.  
   
   Lets start by looking briefly at the most frequently used words with the stop words included. We will remove these after this.  This initial look also spans all three movies.  

```{r R.options=list(max.print=10)}
#most words used without removing stop words
sw_tidy$movies <- base::factor(sw_tidy$movie, levels = base::rev(titles))

sw_tidy

sw_tidy %>%
  dplyr::count(word, sort = TRUE)
```

Unsuprisingly, stop words dominate the top of the word frequency initial look. We can see that the names of some charaters are present event amoung the stop words. Now let us look at the same frequency but after removing stop words.

```{r}
#most commoon words used with removing stop words
sw_tidy %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  dplyr::count(word, sort = TRUE)
```
   
   The top words used in the star wars scripts are primarily charaters. This makes sense as each of the lines of the script begins with the charater who said that line. This is similar to how it would appear in a book however the script provides no information as to the setting the charaters are in.  

   Now we can look at the word frequencies from each of the movies individually.  


```{r}

#most common words used from each movie removing stop 
#desplays the top 10 from each movie
sw_tidy %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  dplyr::group_by(movie) %>%
  dplyr::count(word, sort = TRUE) %>%
  dplyr::top_n(10)

```

Using the ggplot2 package, I am able to illustrate these most commonly used words which does a good job identifying the main charaters for each of the movies.  

```{r}
library(ggplot2)
library(dplyr)
sw_tidy %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  group_by(movie) %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(movie = base::factor(movie, levels = titles),
         text_order = base::nrow(.):1) %>%
  ## Pipe output directly to ggplot
  ggplot(aes(reorder(word, text_order), n, fill = movie)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ movie, scales = "free_y") +
  labs(x = "Word", y = "Frequency") +
  coord_flip() +
  theme(legend.position="none")

```

   This gives ua pretty good idea on who the major charaters are in each of the movies with Luke, Han, and C3PO being in the top for each of the movies. It is hard to gain any additional information from this esspecially since many of the most frequently used words are used in all three movies.    

##Summarization of Each Movie

   Now that we have looked at the word frequencies for each of the movies, we can move on to look at summaries of the movie script.  As mentioned above, these movie scrips are similar to books except the scripts have no descriptions of the environment. There are strictly statements that each charaters says.  Keeping that in mind, it is unrealistic to construct a full summary of the movie using extractive methods. Rather, we will pull out important lines said by charaters in an attempt to jog someones memory of what happens in the movie.  Our goal is to obtain the sequence of events in case someone who has already seen the movie has fogotten the order of events that had taken place throughout the movie.  
   
   First we will have to restructor the data a little bit.  

```{r R.options=list(max.print=10)}
#the same as above but we are creating a new tibble
titles <- c("Episode 4", 
            "Episode 5", 
            "Episode 6")
movies2<-list(Episode4,
             Episode5,
             Episode6)

sw_tidy2 <- tibble::tibble()

#rather than tokenize with words, we will use sentences as our tokenizer so the extraction will be an entire sentense.  
for(i in seq_along(titles)) {
  
  clean <- tibble::tibble(text = movies[[i]]) %>%
    tidytext::unnest_tokens(sentence, text,token ='sentences') %>%
    dplyr::mutate(movie2 = titles[i]) %>%
    dplyr::select(movie2, dplyr::everything())
  
  sw_tidy2 <- base::rbind(sw_tidy2, clean)
}

#sw_tidy2
#This will get rid of all the numbers which were in front of each line of the script. We should no longer need these numbers as they may skew our results. 
sw_tidy2$sentence<-gsub(sw_tidy2$sentence,
      pattern = "[0-9]",
      replacement = ""
      )
#check to make sure it got rid of the numbers.  
sw_tidy2$sentence[1:10]
```
  Now that we have reformatted our data, we can use the lexrankr package to get a summary from the movie.  Because our goal is the get the sequence of events for each of the movies, we will try and find the top three most important lines for each quarter of the movies.  For example we will summarize episode 4 for the first 1/4 lines, then another summarization for the second 1/4 lines and so on.  This will allow us to obtain a sequence of events summary for someone who has already seen the movie but may have forgotten the order of which the events took place.  
```{r}
library("lexRankr")
#this will subset into each of the movies. Not completely nessasary but easier to think through.  
Ep4_tidy<-subset(sw_tidy2,sw_tidy2$movie2=="Episode 4")
Ep5_tidy<-subset(sw_tidy2,sw_tidy2$movie2=="Episode 5")
Ep6_tidy<-subset(sw_tidy2,sw_tidy2$movie2=="Episode 6")
Ep4Summary<-NULL
Ep5Summary<-NULL
Ep6Summary<-NULL
#perform lexrank for top 3 sentences for the quarters of episode 4
for (i in 1:4){
top_3Ep4 = lexRankr::lexRank(Ep4_tidy$sentence[(i-1)*round(nrow(Ep4_tidy)/4)+1:round(nrow(Ep4_tidy)/4)*i],
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, round(length(Ep4_tidy$sentence[1:nrow(Ep4_tidy)])/4)),
                          #return 3 sentences
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in article
order_of_appearance = order(as.integer(gsub("_","",top_3Ep4$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3Ep4[order_of_appearance, "sentence"]
Ep4Summary<-cbind(Ep4Summary,ordered_top_3)
}
Ep4Summary
```

The LexRankr package does a pretty good job in summarizing the different parts for the star wars episode 4 movie! Note: this is the same summarizer than common Reddit summarizes use. 

```{r}

#perform lexrank for top 3 sentences for the quarters of episode 5
for (i in 1:4){
  

top_3Ep5 = lexRankr::lexRank(Ep5_tidy$sentence[(i-1)*round(nrow(Ep5_tidy)/4)+1:round(nrow(Ep5_tidy)/4)*i],
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, round(length(Ep5_tidy$sentence[1:nrow(Ep5_tidy)])/4)),
                          #return 3 sentences
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in movie
order_of_appearance = order(as.integer(gsub("_","",top_3Ep5$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3Ep5[order_of_appearance, "sentence"]
Ep5Summary<-cbind(Ep5Summary,ordered_top_3)
}
Ep5Summary

```

Lexrankr does an alright job summarizing episode 5 but there are sections where it does not perform very well.  This may be because the first part of episode 5 is a larger action sequence which the script is not able to fully grasp. 


```{r}
#perform lexrank for top 3 sentences for the quarters of episode 6
for (i in 1:4){
  

top_3Ep6 = lexRankr::lexRank(Ep6_tidy$sentence[(i-1)*round(nrow(Ep6_tidy)/4)+1:round(nrow(Ep6_tidy)/4)*i],
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, round(length(Ep6_tidy$sentence[1:nrow(Ep6_tidy)])/4)),
                          #return 3 sentences
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in article
order_of_appearance = order(as.integer(gsub("_","",top_3Ep6$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3Ep6[order_of_appearance, "sentence"]
Ep6Summary<-cbind(Ep6Summary,ordered_top_3)
}
Ep6Summary

```

Same as for episode 5, episode 6 does an alright job in some parts but a poor job in summarizing in other parts.  
Likely due to the non-descriptive actions and scenery in the script text files, the lexrankr package is limited on the amount of information it is able to gather strictly from the written script.  

Now we will move on to name entity recognition where we will try and pull out the main charaters and locations from the script.  

##Name Entity Recognition

Now that we have tried to summarize, we are going to attempt to pick out the entities from the script.  
Again, we will start from scratch because of the slightly different formatting required to use the packages for openNLP and NLP.  

```{r}

#most of the data reading is the same.  

files<-list.files("Files", pattern = '.txt',full.names = T)


Episode4<-read_file(files[1])
Episode4<-gsub('\\\"'," ",Episode4)
Episode5<-read_file(files[2])
Episode5<-gsub('\\\"'," ",Episode5)
Episode6<-read_file(files[3])
Episode6<-gsub('\\\"'," ",Episode6)

#makes titles as markers for later use
titles <- c("Episode 4", 
            "Episode 5", 
            "Episode 6")

#creates a single list of the movies
movies<-list(Episode4,
             Episode5,
             Episode6)

#makes the movie as a single string
movies<-as.String(movies)
movies<-gsub(movies, 
      pattern = "[0-9]",
      replacement = "")

```

Next, we need to use the maximum entropy functions in openNLP to annotate the star wars movies document. Additionally we will use sentence and word annotators to further prepare our data.  

```{r}
#we can use the sent and word token annotators to annotate the entire script
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()

#this will annotate the entire script
movies_tokenized <- NLP::annotate(movies, list(sent_token_annotator, word_token_annotator))

person_entity_annotator<-Maxent_Entity_Annotator(kind="person")
location_entity_annotator<-Maxent_Entity_Annotator(kind="location")


# we can now bundle them into one list
annotators_all<-list(person_entity_annotator,location_entity_annotator)


```

Using the person and location annotators, we are able to see how well the packages do in picking out star wars locations and charaters.  

```{r}

people<-movies[annotators_all[[1]](movies,movies_tokenized)]
locations<-movies[annotators_all[[2]](movies,movies_tokenized)]
table(rbind(people,locations))
```

The packages do a decent job in picking out the main charaters tho it is missing a couple. Additionally, there are random words included that the package thought were important descriptors but ended up not being important.  Some limitations for this can be seen. However this package was constructed or trained probably did not have star wars lingo and charaters in their dictionary. This means that the package needed to use different methods for determining if a word was an entity. This lead to several missclassifications. Overall all it did an alright job in finding the charaters and locations.  


#Conclusion
We have now gone through several different text mining analysis techniques with various R packages. We saw the most important words used in the script. We used R packages to try and build summaries for each of the movies. Finally we used NLP and openNLP packages to pull out important charaters and locations from the stat wars movies.  Although there were several limitations to these packages and methods, we were able to gain insight to the star wars movies without actually needing to rewatch the movies which accomplished our initial goal for the project.

#Future Work
If we had more time for the project, it would be interesting to see what other techniques work with the scripts.  Also, we could also look at other sources of data rather than the movie script itself. Potentially we could find better results if we were able to action and scenery descriptions more similar to what is in a book rather than a script. This might provide more important information that would help with our summarization and Name Entity recognition.  

All in all, this was a great learning experience and feel much more comfortable diving into new packages in R.  This has been one of my favorite classes I have taken here at AFIT!

