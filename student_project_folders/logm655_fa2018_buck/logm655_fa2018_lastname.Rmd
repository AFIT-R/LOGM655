---
title: "LOGM 655 Student Project Report"
author: "Lt Jennifer Buck"
date: "December 7, 2018"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = F,
                      warning = F, 
                      comment = NA)
```

Abstract: Text mining can be used to gain a unique perspective and understanding of song lyrics. Natural language processing and text mining techniques were used to discover the themes and emotion behind the lyrics of Young the Giant (YTG). Within this research effort, we discuss the challenges of classifying a band solely on their song lyrics and how to overcome this barrier. The methods used for evaluation were word frequencies analysis, sentiment analysis, and ngram analysis. Overall, these techniques were successful at providing a preliminary look at themes and serve as a stepping stone for more sophisticated analysis.

## Project Overview/Problem Statement 
 
<center>
<img src="Images/Covers.png" alt="Visual" vspace="25"> 
</center>

Young the Giant (YTG) is an american alternative rock band that recently released their fourth album, *Mirror Master*. They released their debut album in 2010, which featured the breakout single "My Body". They have since been my favorite band and with each successive album, I grow to love them even more. I thought it would be fun to explore their lyrics from a text mining perspective, and compare how they have grown as a band over time. Text mining and Natural Language Processing are ways to transforms text documents in a quantitative format used for analysis. For this project, we will dive into the lyrics of YTG using text mining analytics to structure the data and gain insights behind the music. Text mining techniques can quickly give us an understanding of each album. First, we will find the most frequently used words to discover important themes in each song/album. Next, sentiment analysis will be used to look at the emotion behind each songs to give us a feel of each album. Lastly, we will briefly look at bigrams to look at word relationships to see if that will add more meaning in the analysis. These will be the main means of lyrical analysis. 

Using text mining techniques to execute lyrical analysis will be an intersting endeavor because of the aytpical structure of most songs. It is challanging because sentence structures differ from everyday vanacular. Songs are often more whimsical and may have an unstructered or illogical flow. Another challenge to address is the repeated words which potential inflate all metrics. The overall goal of this project is to use NLP to gain unbaised insight into the lyrics of Young the Giant.

## Installing Loading Required R Packages

The following packages were used for the lyrical analysis:

```{r, warning=FALSE, message=FALSE}
pacman::p_load(tm,
               rvest, 
               xml2,
               tidytext, 
               dplyr, 
               ggplot2, 
               magrittr, 
               stringr, 
               tidyr,
               knitr, 
               wordcloud, 
               gridExtra)
```

## Methodology 


__Part one: scrapping the songs from an html.__


I used the packages rvest and xml2 to retrieve all of YTG's the song lyrics from the website AZ lyric. This gave the following results:


```{r, warning=FALSE, message=FALSE}
#Scraps html code 
#example using the song mirror master
scraping_wiki <- read_html("https://www.azlyrics.com/lyrics/youngthegiant/mirrormaster.html")

#obtains all the text from html file. Upon inspection, element 22 has all the lyrics
all_text <- scraping_wiki %>%
  html_nodes("div") %>% 
  html_text()

all_text[22]
```


__Part two: cleaning the data and manipulating it into a workable format.__ 

Now that I have all the lyrics, I needed to manipulate it in a way that read fluently without metacharacters. I used the function,  gsub, to replace "..." with "." because my goal was to seperate lines based on period location. Replaced "\n" to periods, and "\r" to spaces. Then once I got rid of all the weird html characters, and the periods in the correct locations, I was able to strsplit a song based on the periods. I was able to build a tibble that related lines to their respective albums and songs. 



```{r}
#rename each file to its corresponding songs title
mirror_master<-all_text[22]

mirror_master<-gsub("\\...", "", mirror_master)
mirror_master<-gsub("\n", ". ", mirror_master)
mirror_master<-gsub("\\\"", " ", mirror_master)
mirror_master<-gsub("\r.|\r", "", mirror_master)
mirror_master<-unlist(strsplit(mirror_master, "\\."))

#I have random blanks in the format, so I took in the elements that have lyrics. 
mirror_master<-mirror_master[1:46]
mirror_master
```

The last step is creating the tibble to work with. Note: I hastily used string split to seperate each song into corresponding line. Since this is not how I inteded to perform part of my analysis, I had to regroup everything afterwards. However, this was still sufficient to perform word frequency analysis because I would seperate it by word and album. 

__Part three: conduct word frequency analysis, sentiment analysis, and examine word relationships with ngrams.__ 

## Analysis

### Word Frequency

An easy way to get a feel for an album is to look at the most frequently used words in each album. First we read in the dataset, remove common words like "the", "is", "and", etc. These are known as stop words and don't typically don't add context to figuring out what a document/song is about. 

```{r, warning=FALSE, message=FALSE}

#read in project data
ytg_tidy<-readRDS("project_data/ytg_project_data.rds")
YTGA_tidy<-readRDS("project_data/YTGA_tidy.rds")
MOMA_tidy<-readRDS("project_data/MOMA_tidy.rds")
HOTS_tidy<-readRDS("project_data/HOTS_tidy.rds")
MM_tidy<-readRDS("project_data/MM_tidy.rds")

#Combines the album titles to use in loop
covers <- c("Young the Giant", 
            "Mind Over Matter", 
            "Home of the Strange", 
            "Mirror Master")


#creates visual showing top ten words from each album
ytg_tidy %>%
  anti_join(stop_words) %>%
  group_by(album) %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(album = base::factor(album, levels = covers),
         text_order = nrow(.):1) %>%
  # Pipe output directly to ggplot
  ggplot(aes(reorder(word, text_order), n, fill = album)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ album, scales = "free_y") +
  labs(x = "Word", y = "Frequency") +
  coord_flip() +
  theme(legend.position="none")



```



Now that we have the most frequent words for each album, maybe we can figure out themes. Time seems to be a frequent theme between all four albums. There are a lot of words between all albums that describe a moment, for instance, time, night, home, days, forever, live, goodnight, and summer. Describing time periods is an easy way to paint a picture in songs. Feel is also common in the first three albums. Describing sensation and acts are also ways to add meaning to songs (wonderful, wrong, move, dance, falling, call, coming, etc.) The first two albums seem to have more physical concepts (body, eyes, heart, etc.) while the later albums in my opinion can have a deeper meaning (glory, live, grow, forever). This helps to see how YTG has changed over time. I also find it interesting that there aren't a lot of references to "girl/boy" or typical words associated with romance. Besides heart, falling, and love, there seems to be more to their music than a common love song. Lets see if this holds true by looking at words unique to each album. 

####Term vs. Document Frequency

Looking at word frequency is not the most sophisticated method to determine importance. Term frequency-inverse document frequency (tf-idf) is a method of weighting infrequent words higher than frequent words within a collection or corpus of documents
[(1)](https://www.r-bloggers.com/prophets-of-gloom-using-nlp-to-analyze-radiohead-lyrics/). A words importance increases proportional to the number of occurences in an album, but is offset by its frequency in the collection as a whole [(1)]. In that event, a word that is uncommon across all the different albums, but frequent within one album will have a higher weight 

So let's look at the most frequent words using the tf-idf measure

```{r, warning=FALSE, message=FALSE}
#looking at the highest tf-idf top 15 terms

ytg_words <- ytg_tidy %>%
  count(album, word, sort = TRUE) %>%
  dplyr::anti_join(stop_words) %>%
  ungroup()

series_words <- ytg_words %>%
  group_by(album) %>%
  summarise(total = sum(n))

ytg_words <- left_join(ytg_words, series_words)

ytg_words %>%
  mutate(ratio = n / total) %>%
  ggplot(aes(ratio, fill = album)) +
  geom_histogram(show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~ album, ncol = 2)+
  stat_bin(bins = 30)
```

This graph shows the distribution of word _n_ in proportion to the total number of words in an album 

```{r}
#Inverse Document Frequency and tf-idf

ytg_words<-ytg_words %>%
  bind_tf_idf(word, album, n)

ytg_words %>%
  dplyr::arrange(dplyr::desc(tf_idf))

ytg_words %>%
  dplyr::arrange(dplyr::desc(tf_idf)) %>%
  dplyr::mutate(word = base::factor(word, levels = base::rev(base::unique(word))),
                album = base::factor(album, levels = covers)) %>% 
  dplyr::group_by(album) %>%
  dplyr::top_n(12, wt = tf_idf) %>%
  dplyr::ungroup() %>%
  ggplot(aes(word, tf_idf, fill = album)) +
  geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
  labs(title = "Highest tf-idf words in the Young the Giant Albums",
       x = NULL, y = "tf-idf") +
  facet_wrap(~album, ncol = 2, scales = "free") +
  coord_flip()
```



Looking at the tf-idf instead of word frequency, we are able to pick out unique words between the albums. Above, time is a frequent word throughout all the albums, but when we look at the td-idf, none of the albums list time anymore. Thats because time was weighted for appearing in all the albums. Unfortunatly, most of the unique words are song titles (body, cough syrup, anagram, camera, paralyzed, silver tongue, amerika, simplifies, glory, and oblivion.) This makes sense because song titles themselves often foreshadow to what a song is about. Let's add these words to a custom stop words list to see if we can gain more insight from removing them. 

```{r, warning=FALSE, message=FALSE}
custom_stop_words <- bind_rows(data_frame(word = c("body", "cough", "syrup", "anagram", "camera", "paralyzed", "silver", "tongue", "amerika", "simplifies", "glory", "oblivion" ), lexicon=c("custom")), 
                               stop_words)

custom_stop_words
```

Now lets redo tf-idf for each album

```{r}
#Inverse Document Frequency and tf-idf with custom stop words

ytg_new_words <- ytg_tidy %>%
  count(album, word, sort = TRUE) %>%
  dplyr::anti_join(custom_stop_words) %>%
  ungroup()

ytg_new_words<-ytg_new_words %>%
  bind_tf_idf(word, album, n)

ytg_new_words %>%
  dplyr::arrange(dplyr::desc(tf_idf))

ytg_new_words %>%
  dplyr::arrange(dplyr::desc(tf_idf)) %>%
  dplyr::mutate(word = base::factor(word, levels = base::rev(base::unique(word))),
                album = base::factor(album, levels = covers)) %>% 
  dplyr::group_by(album) %>%
  dplyr::top_n(10, wt = tf_idf) %>%
  dplyr::ungroup() %>%
  ggplot(aes(word, tf_idf, fill = album)) +
  geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
  labs(title = "Highest tf-idf words in the Young the Giant Albums",
       x = NULL, y = "tf-idf") +
  facet_wrap(~album, ncol = 2, scales = "free") +
  coord_flip()
```

We can look at both to gain a more accurate summary of each album

The next section is another way to visualize word comparison between albums.

```{r, warning=FALSE, message=FALSE}
# calculate percent of word use across all albums
ytg_pct <- ytg_tidy %>%
  dplyr::anti_join(custom_stop_words) %>%
  dplyr::count(word) %>%
  dplyr::transmute(word, all_words = n / sum(n))

# calculate percent of word use within each album
frequency <- ytg_tidy %>%
  dplyr::anti_join(custom_stop_words) %>%
  dplyr::count(album, word) %>%
  dplyr::mutate(album_words = n / sum(n)) %>%
  dplyr::left_join(ytg_pct) %>%
  dplyr::arrange(dplyr::desc(album_words)) %>%
  dplyr::ungroup()

```

```{r, warning=FALSE, message=FALSE}
#visualize this frequency

ggplot(frequency, 
       aes(x = album_words, 
           y = all_words, 
           color = abs(all_words - album_words))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", 
                       high = "gray75") +
  facet_wrap(~ album, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Young the Giant Albums", x = NULL)
```

Again, this chart lets us compare the frequency for a word within an album against the other albums. Words that are close to line in these plots have similar frequencies across all the albums. Words above the line are common across all the albums but not that particular album while words below the line are common in that particular album but not across the series. An example is time is highly frequent throughout the four albums except for home of the strange, where it lies further above the line. 

###Sentiment Analysis

Sentiment analysis is another way to gain information and a feel for the band. The purpose of this section is to find the emotion behind each song using tidytext. Sentiment analysis was conducted to compliment word frquency analysis, because even though a word is stated frequently, it might not have an impact on the listerner's experience. The sentiment behind a song is often what connects listeners to the music.  

This analysis loosely follows the sentiment analysis in *Text Mining With R* [(2)](https://www.tidytextmining.com/index.html). There are three general-purpose dictionaires or lexicons, for evaluating emotion in text. Most lexicons take in unigrams and the individual words are assigned scores for positive/negative sentiment. Keep in mind that not every word in the English language are in lexicons because many English words are inherently neutral. It is also important to note that because these lexicons only take into account unigrams, it does not take into account qualifiers, for example, “not sad”. This problem will be addressed in the next section when we look at bigrams to look at word relationships. 

Lets use and compare the three different lexicon dictionaries on the newest album, *Mirror Master*. 

```{r, warning=FALSE, message=FALSE } 

#Compare the sentiments between the three different lexicons for the newest album

#AFINN already gives the sentences numeric scores. Bing and NRC need to be transformed numerically to compare
afinn<-MM_tidy %>%
  inner_join(get_sentiments("afinn"))%>%
  group_by(index = song) %>%
  summarise(sentiment = sum(score))%>%
  mutate(method="AFINN")

afinn

bing_and_nrc <- bind_rows(MM_tidy %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          MM_tidy %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = song, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")+
  theme(axis.text.x=element_text(angle=90, hjust =1))
```  

The trends are similar between the three lexicons. The first thing I noticed was that all three lexicons identified glory as the song with the most positive sentiment. Reasoning this out, this is due to the repeated chorus, "glory be to god", which has a collective score of three. This is misleading, because YTG is not a religious band and the meaning behind it is more sultry and mild-tempo. God was also an outcome in the word frequency analysis, which shows that text analysis can be flawed. Sentiment analysis is an interesting way to objectively look at song lyrics. Songs themselves can be misleading, because even though a song is upbeat and catchy, the words could be very dark. This is one reason why lyrical analysis is challanging. 

Now lets look at the top 10 positive and negative words in each album.

```{r, warning=FALSE, message=FALSE}
#2.4 Most common positive and negative words

YTGA_word_counts<-YTGA_tidy %>%
  inner_join(get_sentiments("bing"))%>%
  count(word, sentiment, sort=TRUE) %>%
  ungroup()

MOMA_word_counts<-MOMA_tidy %>%
  inner_join(get_sentiments("bing"))%>%
  count(word, sentiment, sort=TRUE) %>%
  ungroup()

HOTS_word_counts<-HOTS_tidy %>%
  inner_join(get_sentiments("bing"))%>%
  count(word, sentiment, sort=TRUE) %>%
  ungroup()

MM_word_counts<-MM_tidy %>%
  inner_join(get_sentiments("bing"))%>%
  count(word, sentiment, sort=TRUE) %>%
  ungroup()

#creates the word frequency plots
p1 <- YTGA_word_counts %>% 
  group_by(sentiment)%>%
  top_n(10)%>%
  ungroup()%>%
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Young the Giant", 
       x = NULL)+
  coord_flip()

p2 <- MOMA_word_counts %>% 
  group_by(sentiment)%>%
  top_n(10)%>%
  ungroup()%>%
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Mind Over Matter", 
       x = NULL)+
 coord_flip()

p3 <- HOTS_word_counts %>% 
  group_by(sentiment)%>%
  top_n(10)%>%
  ungroup()%>%
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Home of the Strange", 
       x = NULL)+
  coord_flip()

p4 <- MM_word_counts %>% 
  group_by(sentiment)%>%
  top_n(10)%>%
  ungroup()%>%
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Mirror Master", 
       x = NULL)+
  coord_flip()



marrangeGrob(grobs = list(p1,p2, p3, p4),
             nrow = 2, ncol = 1)

```

These words illicit more feelings, than the original word frequency analysis. This gives us an overall feel of what each album is like. 

### Relationships between words

So far, this project has only look at singular words. To gain more fidelity in the themes and sentiment beyond individual words, we need to examine the relationship between words. This next segment tokenizes based on pairs of adjacent words or "ngrams". In this case we will look at bigrams, or two consecutive words. First lets look at the most frequent paired words in the title album. 

```{r, warning=FALSE, message=FALSE}

#Finds all the combinations of bigrams
ytg_bigrams<-ytg_tidy %>%
  unnest_tokens(bigram, word, token = "ngrams", n=2)

#Counts the most frequent bigrams
ytg_bigrams %>%
  count(bigram, sort=TRUE)
```

From first inspection, the resulting bigrams are a combination of stop words that don't add contextual meaning. I also noticed bigrams that are the same word repeated, which is common in musical lyrics. The next step is to seperate the bigrams, which we will call "word1" and "word2", to remove stop words and duplicate words.

```{r, warning=FALSE, message=FALSE}
#seperates the bigrams based on the space between the words
bigrams_separated <- ytg_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")


bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# Find words that are duplicated to filter out
duplicate_bigrams <- bigrams_filtered %>% 
  filter(word1 == word2)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)%>%
  filter(!word1 %in% duplicate_bigrams$word1)

bigram_counts
```

Looking at the bigrams we can provide context for sentiment analysis. For instance, the word break is negative and free is positve, but the phrase break free is overall positive. 

One major problems with performing sentiment analysis on a singular word is the use of negation. The phrase, "not alone" would appear as two negative words, when the overall sentiment of that phrase is positive. Lets look at all the bigrams that contain "not": 


```{r, warning=FALSE, message=FALSE}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")


bigrams_separated%>%
  filter(word1 == "not")%>%
  count(word1, word2, sort = TRUE)


```

Then lets check the scores of the second word to see if there were any inconsistencies:

```{r, warning=FALSE, message=FALSE}
AFINN <- get_sentiments("afinn")

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words
```

This resulted in four words that were giving the wrong sentiment score. Not is a neutral word according the the AFINN lexicon. This skews the sentiment scoring, because the sentence containing not lost, has at least a score of -3, where it should count as 3 because of the negation. With more time, a user could replace words or come up with an algorithm to offset the negation words to get a more accurate sentiment analysis. This is why looking at the word relationships can aid in other analysis techniques. 

## Future Work

This was a good first look at using lyrical analysis to understand the band YTG. If I had more time/didn't go over the 20 page requirement here are the following things I would do. First step would be to rescrub the data. Get rid of repeated lines, because they skew the frequincies and sentiment analysis. Combine more than one word song titles, example, "silver" and "tongue", because they also skew the frequency analysis. Include the custom stop words in more techniques.

Second, dig deeper into why the sentiment analysis didn't match the melody. Songs I believed to be positive or negative were not in line with this portion of the analysis. I believe this could be corrected by conducting sentiment analysis using bigrams to see if that provides a more accurate feel for the songs. 

Lastly, compare the lexical diversity of Young the Giant against Top 50 alternative rock and pop songs. See how to compare to music similar to their own style and popular music. I predict that their lyrics will contain more diversity than popular music, because music nowadays has less substance and the same repeated words. 