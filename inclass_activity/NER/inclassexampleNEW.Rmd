---
title: "LOGM 655 - Examples"
subtitle: "Named Entity Recognition"
author: "MAJ Bryan Kolano, 2d Lt Nate Beveridge, 2d Lt Zachary Kane"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  html_document:
    code_folding: 'hide'
    
---

# Named Entity Recognition using openNLP
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = F,
                      warning = F, 
                      comment = NA,
                      cache = TRUE)

```

In order for openNLP to work, you must first download the following package
```{r , results='hide'}

options( java.parameters = "-Xmx5120m" )
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/",type = "source")
```
Next, load the following packages, or call them from the library.
```{r}

#pacman::p_install(c("openNLP","coreNLP","readr","NLP","monkeylearn","magrittr"))

# Or if you already have them installed, just load the libraries

library(openNLP)
library(NLP)
library(coreNLP)
library(monkeylearn)
library(readr)
library(magrittr)
options( java.parameters = "-Xmx5120m" )
library(rJava)
```


Now that the packages are loaded, we need to insert the text documents

```{r, results='hide'}

rem <- readr::read_file("Remington.txt")
title <- c()
for (i in 1:4){
  j=1989
  title[i] <- paste("Bush_",j,".txt", sep = "")
  title <- c(title)
  j <- j + 1
}

title2 <- c()
for (i in 1:4){
  k=2001
    title2[i] <- paste("Bush_",k,".txt", sep = "")
  title2 <- c(title2)
  k <- k + 1
}


all <- c(title,title2)
speech <- c()
for (i in seq_along(all)){
  
  speech[i] <- read_file(all[i])
  i <- i + 1
}



```



# Named Entity Recognition using coreNLP

## Download coreNLP

```{r}

pacman::p_load("rJava")
#               "coreNLP")
devtools::install_github('statsmaths/coreNLP') 
#install.packages("coreNLP")
```

There is a coreNLP package on CRAN but the only the developmental github version will complete sentiment analysis when I tested between the two.

```{r}
coreNLP::downloadCoreNLP() # creating an instance (opening the pipeline)
coreNLP::initCoreNLP()
str <- c("President George Bush spoke to the troops on board of an aircraft carrier in the Gulf of Mexico.")
library(coreNLP)
output <- coreNLP::annotateString(str)
NER <- coreNLP::getToken(output)
NER[,c("sentence","id","token","POS","NER")]

# word character. Raw word in the input text.
# lemma character. Lemmatized form the token.

```

The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that can easily be understood and effectively used by people without linguistic expertise who want to extract textual relations. In particular, rather than the phrase structure representations that have long dominated in the computational linguistic community, it represents all sentence relationships uniformly as typed dependency relations.The current representation contains approximately 50 grammatical relations (depending slightly on the options discussed in section 4). The dependencies are all binary relations: a grammatical relation holds between a governor (also known as a regent or a head) and a dependent. Further explanation of dependencies can be found in References [(1)](https://nlp.stanford.edu/software/dependencies_manual.pdf).

```{r}
# Returns a data frame of the coreferences of an annotation
coreNLP::getDependency(output)
```

StanfordCoreNLP includes the sentiment tool and various programs which support it. The model can be used to analyze text as part of StanfordCoreNLP by adding “sentiment” to the list of annotators. There is also command line support and model training support.

SentimentAnnotator implements Socher et al’s sentiment model. Attaches a binarized tree of the sentence to the sentence level CoreMap. The nodes of the tree then contain the annotations from RNNCoreAnnotations indicating the predicted class and scores for that subtree [(2)](https://stanfordnlp.github.io/CoreNLP/sentiment.html).

```{r}
coreNLP::getSentiment(output)
```

## Applied to Remington Text

```{r}
rem_output <- coreNLP::annotateString(rem)
rem_output$token
```


```{r}
rem_NER <- coreNLP::getToken(rem_output)
table(rem_NER$NER)
people_rem <- subset(rem_NER, NER=="PERSON")
(people_rem)
```

```{r}
library(tidyr)
rem_sent <- coreNLP::getSentiment(rem_output)
rem_sent
# 21 sentences identified in the beginning
```


# Challenges of Named Entity Recognition

The NER pieces of the different NLP packages worked fairly well for the State of the Union Addresses.  However, using the article from class on Monday, problems arose when attempting to identify persons, locations, and organizations.

```{r}
#first set the document as one string
s <- as.String(rem)

```

Next, we need to use the maximum entropy functions in openNLP to annotate the Remington document.

```{r}
library(openNLP)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()

a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
```

Let's first look at the person entities
```{r}
## Entity recognition for persons.
entity_annotator <- openNLP::Maxent_Entity_Annotator()
annotate(s, entity_annotator, a2)
## Directly:
entity_annotator(s, a2)
## And slice ...
s[entity_annotator(s, a2)]
```

Next, the location entities
```{r}
#Entity recognition for locations
entity_annotator_location <- Maxent_Entity_Annotator(kind = "location")
annotate(s, entity_annotator_location, a2)
## Directly:
entity_annotator_location(s, a2)
## And slice ...
s[entity_annotator_location(s, a2)]

```

Next, the organization entities
```{r}
#Entity Recognition for Organizations
entity_annotator_org <- Maxent_Entity_Annotator(kind = "organization")
annotate(s, entity_annotator_org, a2)
## Directly:
entity_annotator_org(s, a2)
## And slice ...
s[entity_annotator_org(s, a2)]
```

Finally, the date entities
```{r}
#Entity Recognition for dates
entity_annotator_date <- Maxent_Entity_Annotator(kind = "date")
annotate(s, entity_annotator_date, a2)
## Directly:
entity_annotator_date(s, a2)
## And slice ...
s[entity_annotator_date(s, a2)]
```
