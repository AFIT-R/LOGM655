---
title: "LOGM 655 In-Class activity"
subtitle: "Named Entity Recognition"
author: "Your name"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  html_document:
    code_folding: 'hide'
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = F,
                      warning = F, 
                      comment = NA)

```


The goal of Named Entity Recognition (NER) is the process of finding spans of text that consititue a proper names and then classifying the type of entity. Named entities are anything that that can be referred to with a proper noun: a person, a location, or an organization.  They have also been extended to include dates, times, and numerical expressions. Named entities can give insites to events in text, relationship between people in a text, and aid in other text mining techniques. For example, we may want to conduct sentiment analysis towards a certain entity. The following table displays a list of genereic named enitity types with the kind of enitites they refer to. 

<center>
<img src="images/generic_entities.png" alt="Anchor Sequences" vspace="25">
</center>

There are challenges when trying to perform named entity recognition. It is difficult to segment the text correctly to identify what is and isn't an entity. It is also hard to identify the type of entity for a given word. For example, the entity JFK could be used for the airport, a person, or other schools, bridge, and street names. 

#NER as Sequence Labeling

A word-by-word sequence labeling task is the most common algorithm for performing named entity recognition. Sequence labeling is simply trying to assign a label to each element of a sequence chosen from a small fixed set of labels. The assigned tags capture both the boundary and the entity type. There are two formats that entities can be tagged: IOB and IO tagging. IOB tagging introduces a tag for the beginning (B) and inside (I) of each entity type, and one for tokens outside (O). This results in _2n+1_ tags for _n_ enitites. IO tagging doesn't identify the B tags, and thus isn't able to distinguish between two entities of the same type right next to one another. This is more rare so typically IO tags are sufficient. IO tagging is simplier by only producing _n+1_ tags. Below we'll see a sentence broken down into IOB and IO tagged entities. 

[ORG __American Airlines__], a unit of [ORG __AMR Corp.__], immediately matched the move, spokesman [PER __Tim Wagner__] said.

This sentence is tagged and below are the associated IOB and IO labels broken down.

<center>
<img src="images/IOB_and_IO_Labeling.png" alt="Anchor Sequences" vspace="25">
</center>

The next sections all cover the standard algorithms for NER.

###Feature-based NER

Feature-based NER extracts features and then trains either a Maximum Entropy Markov Models (MEMM) or Conditional Random Field (CRF) sequence model. Word shape plays an important role in a feautre based apporach. Word shape features are used to represent the letter pattern of a word by mapping lower-case letters to 'x', upper-case to 'X', numbers to 'd', and retaining punctuation. For example I.M.F would map to X.X.X and DC10-30 would map to XXdd-dd.

Recall that MEMM's compute the posterior $P(T|W)$ directly

Lists are developed from both gazetteers and name-lists to improve the alrotithm. Gazetteers are lists of place names, often providing millions of entries for locations with geographical and political information. Name-lists consist of names, surnames, or corporations, commerical products, and other areas. Gazetteer and name features are implemented as a binary featur for each name list. These lists can difficult to create, maintain, and aren't always useful.  

A sequence classifier like an MEMM can be trained to label new sentences. In the below example, the operation of a sequence labeler is at the point where the token _Corp._ is next to be labeled. Everything in the boxed area is the features availible to the classifier.

<center>
<img src="images/Feature_based_classifier.png" alt="Anchor Sequences" vspace="25">
</center>


###Neural Algorithm for NER

###Rule-based NER

Machine learned (neural or MEMM/CRF) sequnce models are common in academic research, but for commerical use, NER is often based on pragnatic combinations of lists and rules. Rule-based NER will typically make repeated passes over text and uses previous results of one pass to influence the next. 

1. Use high-precision rules to tag unambiguous entity metnions.
2. Search for substring matches of the previosuly detected names.
3. Consult application-specific name lists to identify likely name entity mentions from the given domain.
4. Apply probabilistic sequence labeling techniques that make use of the tags from previosu stages as additional features.



###Evaluation Methods

NER systems are evaluated by recall, precision, and $F_1$ measure. Recall is the ratio fo the number of correcetly labeled responses to the total that should have been labeled. Precision is the ratio of the number of correctly labeled respsonses to the total labeled. The F-measure is the harmonic mean of the two. 



